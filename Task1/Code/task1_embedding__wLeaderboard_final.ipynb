{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1-ki6sGojv8nenpZbbTizDBGNNWDrkQMD","timestamp":1754716844541},{"file_id":"1xrsexTX_7vQoq156OgssL1hvMSoFBDYI","timestamp":1754674996508},{"file_id":"1jm_ifII7JTJS6iy_kMRx3XJoI4P0QJRy","timestamp":1754674472236}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **embeddings**"],"metadata":{"id":"JChbYbsx0b2u"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"SExBaTAdnzjM"},"outputs":[],"source":["import pandas as pd\n","from sklearn.preprocessing import StandardScaler, LabelEncoder\n","test_form = pd.read_csv('TASK1_test_set_Submission_form.csv', encoding='latin1')\n","x_df_lasso=pd.read_csv('X_df_pca_99.csv')\n","y_df=pd.read_csv('y_df.csv')\n","X_test=pd.read_csv('X_test_pca_99.csv')\n","\n","target_cols = [\n","    'Green', 'Cucumber', 'Herbal', 'Mint', 'Woody', 'Pine', 'Floral',\n","    'Powdery', 'Fruity', 'Citrus', 'Tropical', 'Berry', 'Peach', 'Sweet',\n","    'Caramellic', 'Vanilla', 'BrownSpice', 'Smoky', 'Burnt', 'Roasted',\n","    'Grainy', 'Meaty', 'Nutty', 'Fatty', 'Coconut', 'Waxy', 'Dairy',\n","    'Buttery', 'Cheesy', 'Sour', 'Fermented', 'Sulfurous', 'Garlic.Onion',\n","    'Earthy', 'Mushroom', 'Musty', 'Ammonia', 'Fishy', 'Fecal',\n","    'Rotten.Decay', 'Rubber', 'Phenolic', 'Animal', 'Medicinal',\n","    'Cooling', 'Sharp', 'Chlorine', 'Alcoholic', 'Plastic', 'Ozone', 'Metallic'\n","]"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","class PearsonCosineLoss(nn.Module):\n","    def __init__(self, alpha=0.5, eps=1e-8):\n","        super().__init__()\n","        self.alpha = alpha\n","        self.eps = eps\n","        self.cosine_similarity = nn.CosineSimilarity(dim=1, eps=eps)\n","\n","    def forward(self, y_pred, y_true):\n","        # Pearson Correlation (1 - correlation for minimization)\n","        y_pred_centered = y_pred - y_pred.mean(dim=1, keepdim=True)\n","        y_true_centered = y_true - y_true.mean(dim=1, keepdim=True)\n","        numerator = (y_pred_centered * y_true_centered).sum(dim=1)\n","        denominator = (y_pred_centered.pow(2).sum(dim=1) * y_true_centered.pow(2).sum(dim=1)).sqrt() + self.eps\n","        pearson_corr = numerator / denominator\n","        pearson_loss = 1 - pearson_corr\n","\n","        # Cosine Similarity (1 - similarity for minimization)\n","        cosine_loss = 1 - self.cosine_similarity(y_pred, y_true)\n","        loss = self.alpha * pearson_loss + (1 - self.alpha) * cosine_loss\n","        return loss.mean()\n","class HyperbolicModel(nn.Module):\n","    def __init__(self, input_dim, output_dim):\n","        super().__init__()\n","        self.fc1 = nn.Linear(input_dim, 128)\n","        self.fc2 = nn.Linear(128, 64)\n","        self.fc3 = nn.Linear(64, output_dim)\n","        self.tanh = nn.Tanh()\n","    def forward(self, x):\n","        x = self.tanh(self.fc1(x))\n","        x = self.tanh(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n","import numpy as np\n","\n","def calculate_metrics(y_true, y_pred):\n","    mse = mean_squared_error(y_true, y_pred)\n","    mae = mean_absolute_error(y_true, y_pred)\n","    r2 = r2_score(y_true, y_pred)\n","    corrs = [np.corrcoef(y_true[:,i], y_pred[:,i])[0,1] if np.std(y_true[:,i])>0 and np.std(y_pred[:,i])>0 else 0 for i in range(y_true.shape[1])]\n","    avg_pearson = np.mean(corrs)\n","    norms_true = np.linalg.norm(y_true, axis=1)\n","    norms_pred = np.linalg.norm(y_pred, axis=1)\n","    cos = np.sum(y_true * y_pred, axis=1) / (norms_true * norms_pred + 1e-8)\n","    avg_cosine = np.mean(cos)\n","    return {\n","        'MSE': mse,\n","        'MAE': mae,\n","        'R2': r2,\n","        'Avg Pearson Correlation': avg_pearson,\n","        'Avg Cosine Similarity': avg_cosine\n","    }\n","\n","# Training routine\n","# Training routine\n","def train_and_evaluate(X, y, test_ratio=0.2, epochs=100, lr=1e-3, batch_size=32, alpha=0.5):\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    # Split\n","    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_ratio, random_state=42)\n","    X_train_t = torch.FloatTensor(X_train).to(device)\n","    y_train_t = torch.FloatTensor(y_train).to(device)\n","    X_val_t = torch.FloatTensor(X_val).to(device)\n","    y_val_t = torch.FloatTensor(y_val).to(device)\n","\n","    # Model\n","    model = HyperbolicModel(X.shape[1], y.shape[1]).to(device)\n","    loss_fn = PearsonCosineLoss(alpha=alpha)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","\n","    n = X_train.shape[0]\n","    steps_per_epoch = int(np.ceil(n / batch_size))\n","\n","    # Training loop\n","    for epoch in range(epochs):\n","        model.train()\n","        idx = np.random.permutation(n)\n","        for i in range(steps_per_epoch):\n","            batch_idx = idx[i*batch_size:(i+1)*batch_size]\n","            xb = X_train_t[batch_idx]\n","            yb = y_train_t[batch_idx]\n","            optimizer.zero_grad()\n","            y_pred = model(xb)\n","            loss = loss_fn(y_pred, yb)\n","            loss.backward()\n","            optimizer.step()\n","        if (epoch+1) % 20 == 0:\n","            print(f\"Epoch {epoch+1}/{epochs}: Train Loss = {loss.item():.4f}\")\n","\n","    model.eval()\n","    with torch.no_grad():\n","        y_train_pred = model(X_train_t).cpu().numpy()\n","        y_val_pred = model(X_val_t).cpu().numpy()\n","    y_train_pred = np.clip(y_train_pred, 0.0, 5.0)  # Clip predictions to valid range\n","    y_val_pred = np.clip(y_val_pred, 0.0, 5.0)\n","\n","    # Print metrics on split data\n","    train_metrics = calculate_metrics(y_train, y_train_pred)\n","    val_metrics = calculate_metrics(y_val, y_val_pred)\n","    print(\"\\\\nTrain Metrics:\", train_metrics)\n","    print(\"Validation Metrics:\", val_metrics)\n","\n","    # Retrain on full data\n","    model = HyperbolicModel(X.shape[1], y.shape[1]).to(device)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","    X_full_t = torch.FloatTensor(X).to(device)\n","    y_full_t = torch.FloatTensor(y).to(device)\n","    for epoch in range(epochs):\n","        optimizer.zero_grad()\n","        y_pred = model(X_full_t)\n","        loss = loss_fn(y_pred, y_full_t)\n","        loss.backward()\n","        optimizer.step()\n","    model.eval()\n","    with torch.no_grad():\n","        y_full_pred = model(X_full_t).cpu().numpy()\n","    y_full_pred = np.clip(y_full_pred, 0.0, 5.0)\n","    full_metrics = calculate_metrics(y, y_full_pred)\n","    print(\"\\\\nMetrics on Full Data:\", full_metrics)\n","    return model, device\n","# Run training and print metrics\n","model, device = train_and_evaluate(\n","    x_df_lasso.to_numpy(dtype=np.float32),\n","    y_df.to_numpy(dtype=np.float32),\n","    test_ratio=0.2,\n","    epochs=100,\n","    lr=1e-3,\n","    batch_size=32,\n","    alpha=0.5\n",")\n","\n","# Convert all columns to numeric (with NaNs where conversion failed)\n","X_test_numeric = X_test.apply(pd.to_numeric, errors='coerce')\n","\n","# Fill NaN values (e.g., with 0 or other imputation strategy)\n","X_test_filled = X_test_numeric.fillna(0)\n","\n","# Convert to numpy array of floats\n","X_test_arr = X_test_filled.to_numpy(dtype=np.float32)\n","with torch.no_grad():\n","    X_test_tensor = torch.FloatTensor(X_test_arr).to(device)\n","    test_preds = model(X_test_tensor).cpu().numpy()\n","test_preds = np.clip(test_preds, 0.0, 5.0)\n","# Save submission\n","test_submission = test_form[['stimulus']].copy()\n","for i, col in enumerate(target_cols):\n","    test_submission[col] = test_preds[:, i]\n","\n","test_submission.to_csv('hyperbolic_task1_embedding.csv', index=False)\n","print(\"Test predictions saved.\")\n","print(f\"Test submission shape: {test_submission.shape}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OKn1rTrhN47u","executionInfo":{"status":"ok","timestamp":1754679466546,"user_tz":-330,"elapsed":13558,"user":{"displayName":"Sheerin Irfaanaa M","userId":"08172659908996108299"}},"outputId":"8e661b18-3546-47af-fd58-4d3d32073924"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 20/100: Train Loss = 0.2575\n","Epoch 40/100: Train Loss = 0.1791\n","Epoch 60/100: Train Loss = 0.1801\n","Epoch 80/100: Train Loss = 0.1889\n","Epoch 100/100: Train Loss = 0.1670\n","\\nTrain Metrics: {'MSE': 0.047843001782894135, 'MAE': 0.10741794109344482, 'R2': 0.38044607639312744, 'Avg Pearson Correlation': np.float64(0.74936325723905), 'Avg Cosine Similarity': np.float32(0.8977691)}\n","Validation Metrics: {'MSE': 0.07101926952600479, 'MAE': 0.1415756195783615, 'R2': 0.054527975618839264, 'Avg Pearson Correlation': np.float64(0.40520101481197157), 'Avg Cosine Similarity': np.float32(0.6947701)}\n","\\nMetrics on Full Data: {'MSE': 0.050756826996803284, 'MAE': 0.13093826174736023, 'R2': 0.3084847629070282, 'Avg Pearson Correlation': np.float64(0.5700784523205753), 'Avg Cosine Similarity': np.float32(0.8182753)}\n","Test predictions saved.\n","Test submission shape: (31, 52)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"cqPpiX0yx1zv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.multioutput import MultiOutputRegressor\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n","\n","# --- Custom metrics ---\n","def pearson_correlation_score(y_true, y_pred):\n","    corrs = []\n","    for i in range(y_true.shape[1]):\n","        corr = np.corrcoef(y_true[:, i], y_pred[:, i])[0, 1]\n","        if np.isnan(corr): corr = 0\n","        corrs.append(corr)\n","    return np.mean(corrs)\n","\n","def cosine_similarity_score(y_true, y_pred):\n","    similarities = []\n","    for i in range(y_true.shape[0]):\n","        y_true_norm = y_true[i] / (np.linalg.norm(y_true[i]) + 1e-8)\n","        y_pred_norm = y_pred[i] / (np.linalg.norm(y_pred[i]) + 1e-8)\n","        similarity = np.dot(y_true_norm, y_pred_norm)\n","        similarities.append(similarity)\n","    return np.mean(similarities)\n","\n","def calculate_metrics(y_true, y_pred):\n","    return {\n","        \"MSE\": mean_squared_error(y_true, y_pred),\n","        \"MAE\": mean_absolute_error(y_true, y_pred),\n","        \"R2\": r2_score(y_true, y_pred),\n","        \"Avg Pearson Correlation\": pearson_correlation_score(y_true, y_pred),\n","        \"Avg Cosine Similarity\": cosine_similarity_score(y_true, y_pred),\n","    }\n","\n","# --------- Example workflow ---------\n","# These need to be set before running:\n","# X_df: DataFrame of features, y_df: DataFrame of targets (n_samples, n_targets)\n","# X_test_df: DataFrame of test features\n","# test_form: DataFrame with 'stimulus' column, matches rows of X_test_df\n","# target_cols: list of 51 string column names\n","\n","# 1. Convert to numpy if needed\n","X = x_df_lasso.apply(pd.to_numeric, errors='coerce').fillna(0).to_numpy(dtype=np.float32)\n","y = y_df.apply(pd.to_numeric, errors='coerce').fillna(0).to_numpy(dtype=np.float32)\n","\n","X_test = X_test.to_numpy(dtype=np.float32)\n","\n","# 2. 80/20 split\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# 3. Train MultiOutput RandomForest\n","rf = MultiOutputRegressor(RandomForestRegressor(n_estimators=100, max_depth=None, random_state=42, n_jobs=-1))\n","rf.fit(X_train, y_train)\n","\n","# 4. Print metrics on splits\n","y_train_pred = rf.predict(X_train)\n","y_val_pred = rf.predict(X_val)\n","y_train_pred = np.clip(y_train_pred, 0.0, 5.0)\n","y_val_pred = np.clip(y_val_pred, 0.0, 5.0)\n","print(\"Training metrics:\")\n","print(calculate_metrics(y_train, y_train_pred))\n","print(\"\\nValidation metrics:\")\n","print(calculate_metrics(y_val, y_val_pred))\n","\n","# 5. Retrain on entire dataset, print metrics\n","rf_full = MultiOutputRegressor(RandomForestRegressor(n_estimators=100, max_depth=None, random_state=42, n_jobs=-1))\n","rf_full.fit(X, y)\n","y_full_pred = rf_full.predict(X)\n","y_full_pred = np.clip(y_full_pred, 0.0, 5.0)\n","print(\"\\nFull dataset metrics:\")\n","print(calculate_metrics(y, y_full_pred))\n","\n","# 6. Predict on test set, save file\n","test_preds = rf_full.predict(X_test)\n","test_preds = np.clip(test_preds, 1.0, 5.0)\n","test_submission = test_form[['stimulus']].copy()\n","for i, col in enumerate(target_cols):\n","    test_submission[col] = test_preds[:, i]\n","test_submission.to_csv('rf_corr_cosine_metrics_test_task1_embedding.csv', index=False)\n","print(\"Test predictions saved.\")\n","print(f\"Test submission shape: {test_submission.shape}\")\n"],"metadata":{"id":"dPwlXa2e5Otw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1754679528682,"user_tz":-330,"elapsed":62098,"user":{"displayName":"Sheerin Irfaanaa M","userId":"08172659908996108299"}},"outputId":"2e749189-5c23-4383-9048-abf31190ddf4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training metrics:\n","{'MSE': 0.00927801480761372, 'MAE': 0.055860927869745336, 'R2': 0.8740996927042891, 'Avg Pearson Correlation': np.float64(0.9575483319415488), 'Avg Cosine Similarity': np.float64(0.9636970071574693)}\n","\n","Validation metrics:\n","{'MSE': 0.06800437938968518, 'MAE': 0.1560009736957729, 'R2': 0.023485730781367842, 'Avg Pearson Correlation': np.float64(0.39153178502937036), 'Avg Cosine Similarity': np.float64(0.7064801040328483)}\n","\n","Full dataset metrics:\n","{'MSE': 0.008843468018658887, 'MAE': 0.05496850795634174, 'R2': 0.8798467912568035, 'Avg Pearson Correlation': np.float64(0.9576583310472214), 'Avg Cosine Similarity': np.float64(0.9647196750906709)}\n","Test predictions saved.\n","Test submission shape: (31, 52)\n"]}]},{"cell_type":"markdown","source":["# **Embedding-95+selectkbest**"],"metadata":{"id":"uOBzalAr0ovG"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"NhQeOg-f0oPB"},"outputs":[],"source":["import pandas as pd\n","from sklearn.preprocessing import StandardScaler, LabelEncoder\n","test_form = pd.read_csv('TASK1_test_set_Submission_form.csv', encoding='latin1')\n","x_train_selected=pd.read_csv('x_train_selectkbest.csv')\n","x_test_selected=pd.read_csv('X_test_selectedkbest.csv')\n","x_train_embed=pd.read_csv('X_df_pca_95.csv')\n","x_test_embed=pd.read_csv('X_test_pca_95.csv')\n","y_df=pd.read_csv('y_df.csv')\n","\n","# Concatenate column-wise (axis=1)\n","x_df_lasso= pd.concat([x_train_selected, x_train_embed], axis=1)\n","X_test= pd.concat([x_test_selected, x_test_embed], axis=1)\n","# Save to new CSV\n","\n","\n","target_cols = [\n","    'Green', 'Cucumber', 'Herbal', 'Mint', 'Woody', 'Pine', 'Floral',\n","    'Powdery', 'Fruity', 'Citrus', 'Tropical', 'Berry', 'Peach', 'Sweet',\n","    'Caramellic', 'Vanilla', 'BrownSpice', 'Smoky', 'Burnt', 'Roasted',\n","    'Grainy', 'Meaty', 'Nutty', 'Fatty', 'Coconut', 'Waxy', 'Dairy',\n","    'Buttery', 'Cheesy', 'Sour', 'Fermented', 'Sulfurous', 'Garlic.Onion',\n","    'Earthy', 'Mushroom', 'Musty', 'Ammonia', 'Fishy', 'Fecal',\n","    'Rotten.Decay', 'Rubber', 'Phenolic', 'Animal', 'Medicinal',\n","    'Cooling', 'Sharp', 'Chlorine', 'Alcoholic', 'Plastic', 'Ozone', 'Metallic'\n","]"]},{"cell_type":"code","source":[],"metadata":{"id":"CzRs9aW408SE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","class PearsonCosineLoss(nn.Module):\n","    def __init__(self, alpha=0.5, eps=1e-8):\n","        super().__init__()\n","        self.alpha = alpha\n","        self.eps = eps\n","        self.cosine_similarity = nn.CosineSimilarity(dim=1, eps=eps)\n","\n","    def forward(self, y_pred, y_true):\n","        # Pearson Correlation (1 - correlation for minimization)\n","        y_pred_centered = y_pred - y_pred.mean(dim=1, keepdim=True)\n","        y_true_centered = y_true - y_true.mean(dim=1, keepdim=True)\n","        numerator = (y_pred_centered * y_true_centered).sum(dim=1)\n","        denominator = (y_pred_centered.pow(2).sum(dim=1) * y_true_centered.pow(2).sum(dim=1)).sqrt() + self.eps\n","        pearson_corr = numerator / denominator\n","        pearson_loss = 1 - pearson_corr\n","\n","        # Cosine Similarity (1 - similarity for minimization)\n","        cosine_loss = 1 - self.cosine_similarity(y_pred, y_true)\n","        loss = self.alpha * pearson_loss + (1 - self.alpha) * cosine_loss\n","        return loss.mean()\n","class HyperbolicModel(nn.Module):\n","    def __init__(self, input_dim, output_dim):\n","        super().__init__()\n","        self.fc1 = nn.Linear(input_dim, 128)\n","        self.fc2 = nn.Linear(128, 64)\n","        self.fc3 = nn.Linear(64, output_dim)\n","        self.tanh = nn.Tanh()\n","    def forward(self, x):\n","        x = self.tanh(self.fc1(x))\n","        x = self.tanh(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n","import numpy as np\n","\n","def calculate_metrics(y_true, y_pred):\n","    mse = mean_squared_error(y_true, y_pred)\n","    mae = mean_absolute_error(y_true, y_pred)\n","    r2 = r2_score(y_true, y_pred)\n","    corrs = [np.corrcoef(y_true[:,i], y_pred[:,i])[0,1] if np.std(y_true[:,i])>0 and np.std(y_pred[:,i])>0 else 0 for i in range(y_true.shape[1])]\n","    avg_pearson = np.mean(corrs)\n","    norms_true = np.linalg.norm(y_true, axis=1)\n","    norms_pred = np.linalg.norm(y_pred, axis=1)\n","    cos = np.sum(y_true * y_pred, axis=1) / (norms_true * norms_pred + 1e-8)\n","    avg_cosine = np.mean(cos)\n","    return {\n","        'MSE': mse,\n","        'MAE': mae,\n","        'R2': r2,\n","        'Avg Pearson Correlation': avg_pearson,\n","        'Avg Cosine Similarity': avg_cosine\n","    }\n","\n","# Training routine\n","# Training routine\n","def train_and_evaluate(X, y, test_ratio=0.2, epochs=100, lr=1e-3, batch_size=32, alpha=0.5):\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    # Split\n","    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_ratio, random_state=42)\n","    X_train_t = torch.FloatTensor(X_train).to(device)\n","    y_train_t = torch.FloatTensor(y_train).to(device)\n","    X_val_t = torch.FloatTensor(X_val).to(device)\n","    y_val_t = torch.FloatTensor(y_val).to(device)\n","\n","    # Model\n","    model = HyperbolicModel(X.shape[1], y.shape[1]).to(device)\n","    loss_fn = PearsonCosineLoss(alpha=alpha)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","\n","    n = X_train.shape[0]\n","    steps_per_epoch = int(np.ceil(n / batch_size))\n","\n","    # Training loop\n","    for epoch in range(epochs):\n","        model.train()\n","        idx = np.random.permutation(n)\n","        for i in range(steps_per_epoch):\n","            batch_idx = idx[i*batch_size:(i+1)*batch_size]\n","            xb = X_train_t[batch_idx]\n","            yb = y_train_t[batch_idx]\n","            optimizer.zero_grad()\n","            y_pred = model(xb)\n","            loss = loss_fn(y_pred, yb)\n","            loss.backward()\n","            optimizer.step()\n","        if (epoch+1) % 20 == 0:\n","            print(f\"Epoch {epoch+1}/{epochs}: Train Loss = {loss.item():.4f}\")\n","\n","    model.eval()\n","    with torch.no_grad():\n","        y_train_pred = model(X_train_t).cpu().numpy()\n","        y_val_pred = model(X_val_t).cpu().numpy()\n","    y_train_pred = np.clip(y_train_pred, 0.0, 5.0)  # Clip predictions to valid range\n","    y_val_pred = np.clip(y_val_pred, 0.0, 5.0)\n","\n","    # Print metrics on split data\n","    train_metrics = calculate_metrics(y_train, y_train_pred)\n","    val_metrics = calculate_metrics(y_val, y_val_pred)\n","    print(\"\\\\nTrain Metrics:\", train_metrics)\n","    print(\"Validation Metrics:\", val_metrics)\n","\n","    # Retrain on full data\n","    model = HyperbolicModel(X.shape[1], y.shape[1]).to(device)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","    X_full_t = torch.FloatTensor(X).to(device)\n","    y_full_t = torch.FloatTensor(y).to(device)\n","    for epoch in range(epochs):\n","        optimizer.zero_grad()\n","        y_pred = model(X_full_t)\n","        loss = loss_fn(y_pred, y_full_t)\n","        loss.backward()\n","        optimizer.step()\n","    model.eval()\n","    with torch.no_grad():\n","        y_full_pred = model(X_full_t).cpu().numpy()\n","    y_full_pred = np.clip(y_full_pred, 0.0, 5.0)\n","    full_metrics = calculate_metrics(y, y_full_pred)\n","    print(\"\\\\nMetrics on Full Data:\", full_metrics)\n","    return model, device\n","# Run training and print metrics\n","model, device = train_and_evaluate(\n","    x_df_lasso.to_numpy(dtype=np.float32),\n","    y_df.to_numpy(dtype=np.float32),\n","    test_ratio=0.2,\n","    epochs=100,\n","    lr=1e-3,\n","    batch_size=32,\n","    alpha=0.5\n",")\n","\n","# Convert all columns to numeric (with NaNs where conversion failed)\n","X_test_numeric = X_test.apply(pd.to_numeric, errors='coerce')\n","\n","# Fill NaN values (e.g., with 0 or other imputation strategy)\n","X_test_filled = X_test_numeric.fillna(0)\n","\n","# Convert to numpy array of floats\n","X_test_arr = X_test_filled.to_numpy(dtype=np.float32)\n","with torch.no_grad():\n","    X_test_tensor = torch.FloatTensor(X_test_arr).to(device)\n","    test_preds = model(X_test_tensor).cpu().numpy()\n","test_preds = np.clip(test_preds, 0.0, 5.0)\n","# Save submission\n","test_submission = test_form[['stimulus']].copy()\n","for i, col in enumerate(target_cols):\n","    test_submission[col] = test_preds[:, i]\n","\n","test_submission.to_csv('hyperbolic_task1_embeddingselected.csv', index=False)\n","print(\"Test predictions saved.\")\n","print(f\"Test submission shape: {test_submission.shape}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1754679531178,"user_tz":-330,"elapsed":2486,"user":{"displayName":"Sheerin Irfaanaa M","userId":"08172659908996108299"}},"outputId":"41238ccd-3aae-4147-fd8d-1bde630ed51f","id":"5O2yOucz08h3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 20/100: Train Loss = 0.4311\n","Epoch 40/100: Train Loss = 0.4328\n","Epoch 60/100: Train Loss = 0.4335\n","Epoch 80/100: Train Loss = 0.4059\n","Epoch 100/100: Train Loss = 0.4024\n","\\nTrain Metrics: {'MSE': 0.10325713455677032, 'MAE': 0.2337791472673416, 'R2': -0.34569546580314636, 'Avg Pearson Correlation': np.float64(0.2886561941544205), 'Avg Cosine Similarity': np.float32(0.6670556)}\n","Validation Metrics: {'MSE': 0.11712990701198578, 'MAE': 0.24456055462360382, 'R2': -0.701462984085083, 'Avg Pearson Correlation': np.float64(0.18416092605619877), 'Avg Cosine Similarity': np.float32(0.62725985)}\n","\\nMetrics on Full Data: {'MSE': 0.1024453416466713, 'MAE': 0.2288665771484375, 'R2': -0.34017738699913025, 'Avg Pearson Correlation': np.float64(0.2932004899691275), 'Avg Cosine Similarity': np.float32(0.6651098)}\n","Test predictions saved.\n","Test submission shape: (31, 52)\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.multioutput import MultiOutputRegressor\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n","\n","# --- Custom metrics ---\n","def pearson_correlation_score(y_true, y_pred):\n","    corrs = []\n","    for i in range(y_true.shape[1]):\n","        corr = np.corrcoef(y_true[:, i], y_pred[:, i])[0, 1]\n","        if np.isnan(corr): corr = 0\n","        corrs.append(corr)\n","    return np.mean(corrs)\n","\n","def cosine_similarity_score(y_true, y_pred):\n","    similarities = []\n","    for i in range(y_true.shape[0]):\n","        y_true_norm = y_true[i] / (np.linalg.norm(y_true[i]) + 1e-8)\n","        y_pred_norm = y_pred[i] / (np.linalg.norm(y_pred[i]) + 1e-8)\n","        similarity = np.dot(y_true_norm, y_pred_norm)\n","        similarities.append(similarity)\n","    return np.mean(similarities)\n","\n","def calculate_metrics(y_true, y_pred):\n","    return {\n","        \"MSE\": mean_squared_error(y_true, y_pred),\n","        \"MAE\": mean_absolute_error(y_true, y_pred),\n","        \"R2\": r2_score(y_true, y_pred),\n","        \"Avg Pearson Correlation\": pearson_correlation_score(y_true, y_pred),\n","        \"Avg Cosine Similarity\": cosine_similarity_score(y_true, y_pred),\n","    }\n","\n","# --------- Example workflow ---------\n","# These need to be set before running:\n","# X_df: DataFrame of features, y_df: DataFrame of targets (n_samples, n_targets)\n","# X_test_df: DataFrame of test features\n","# test_form: DataFrame with 'stimulus' column, matches rows of X_test_df\n","# target_cols: list of 51 string column names\n","\n","# 1. Convert to numpy if needed\n","X = x_df_lasso.apply(pd.to_numeric, errors='coerce').fillna(0).to_numpy(dtype=np.float32)\n","y = y_df.apply(pd.to_numeric, errors='coerce').fillna(0).to_numpy(dtype=np.float32)\n","\n","X_test = X_test.to_numpy(dtype=np.float32)\n","\n","# 2. 80/20 split\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# 3. Train MultiOutput RandomForest\n","rf = MultiOutputRegressor(RandomForestRegressor(n_estimators=100, max_depth=None, random_state=42, n_jobs=-1))\n","rf.fit(X_train, y_train)\n","\n","# 4. Print metrics on splits\n","y_train_pred = rf.predict(X_train)\n","y_val_pred = rf.predict(X_val)\n","y_train_pred = np.clip(y_train_pred, 0.0, 5.0)\n","y_val_pred = np.clip(y_val_pred, 0.0, 5.0)\n","print(\"Training metrics:\")\n","print(calculate_metrics(y_train, y_train_pred))\n","print(\"\\nValidation metrics:\")\n","print(calculate_metrics(y_val, y_val_pred))\n","\n","# 5. Retrain on entire dataset, print metrics\n","rf_full = MultiOutputRegressor(RandomForestRegressor(n_estimators=100, max_depth=None, random_state=42, n_jobs=-1))\n","rf_full.fit(X, y)\n","y_full_pred = rf_full.predict(X)\n","y_full_pred = np.clip(y_full_pred, 0.0, 5.0)\n","print(\"\\nFull dataset metrics:\")\n","print(calculate_metrics(y, y_full_pred))\n","\n","# 6. Predict on test set, save file\n","test_preds = rf_full.predict(X_test)\n","test_preds = np.clip(test_preds, 0.0, 5.0)\n","test_submission = test_form[['stimulus']].copy()\n","for i, col in enumerate(target_cols):\n","    test_submission[col] = test_preds[:, i]\n","test_submission.to_csv('rf_corr_cosine_metrics_test_task1_embeddingselected.csv', index=False)\n","print(\"Test predictions saved.\")\n","print(f\"Test submission shape: {test_submission.shape}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1754679630562,"user_tz":-330,"elapsed":99366,"user":{"displayName":"Sheerin Irfaanaa M","userId":"08172659908996108299"}},"outputId":"45c3a06c-4a69-4c98-eedc-dee26dcae133","id":"irhKMBNd1BpB"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training metrics:\n","{'MSE': 0.008402704929477646, 'MAE': 0.05282630689073382, 'R2': 0.8845786352324458, 'Avg Pearson Correlation': np.float64(0.9577990504929402), 'Avg Cosine Similarity': np.float64(0.9644575071489448)}\n","\n","Validation metrics:\n","{'MSE': 0.05916221108819572, 'MAE': 0.14254284312252002, 'R2': 0.10629212019273619, 'Avg Pearson Correlation': np.float64(0.48342468344308087), 'Avg Cosine Similarity': np.float64(0.7423908000688713)}\n","\n","Full dataset metrics:\n","{'MSE': 0.008172921013057338, 'MAE': 0.052325147409909824, 'R2': 0.8877984058434862, 'Avg Pearson Correlation': np.float64(0.956252911769177), 'Avg Cosine Similarity': np.float64(0.9651906774997744)}\n","Test predictions saved.\n","Test submission shape: (31, 52)\n"]}]},{"cell_type":"markdown","source":["# **embedding+pcalasso**\n"],"metadata":{"id":"262B1jDr4vqe"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"M_vJE1p36D78"},"outputs":[],"source":["import pandas as pd\n","from sklearn.preprocessing import StandardScaler, LabelEncoder\n","test_form = pd.read_csv('TASK1_test_set_Submission_form.csv', encoding='latin1')\n","x_train_selected=pd.read_csv('x_df_lasso_pca.csv')\n","x_test_selected=pd.read_csv('X_test_pcalasso.csv')\n","x_train_embed=pd.read_csv('X_df_pca_95.csv')\n","x_test_embed=pd.read_csv('X_test_pca_95.csv')\n","y_df=pd.read_csv('y_df.csv')\n","\n","# Concatenate column-wise (axis=1)\n","x_df_lasso= pd.concat([x_train_selected, x_train_embed], axis=1)\n","X_test= pd.concat([x_test_selected, x_test_embed], axis=1)\n","# Save to new CSV\n","\n","\n","target_cols = [\n","    'Green', 'Cucumber', 'Herbal', 'Mint', 'Woody', 'Pine', 'Floral',\n","    'Powdery', 'Fruity', 'Citrus', 'Tropical', 'Berry', 'Peach', 'Sweet',\n","    'Caramellic', 'Vanilla', 'BrownSpice', 'Smoky', 'Burnt', 'Roasted',\n","    'Grainy', 'Meaty', 'Nutty', 'Fatty', 'Coconut', 'Waxy', 'Dairy',\n","    'Buttery', 'Cheesy', 'Sour', 'Fermented', 'Sulfurous', 'Garlic.Onion',\n","    'Earthy', 'Mushroom', 'Musty', 'Ammonia', 'Fishy', 'Fecal',\n","    'Rotten.Decay', 'Rubber', 'Phenolic', 'Animal', 'Medicinal',\n","    'Cooling', 'Sharp', 'Chlorine', 'Alcoholic', 'Plastic', 'Ozone', 'Metallic'\n","]"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","class PearsonCosineLoss(nn.Module):\n","    def __init__(self, alpha=0.5, eps=1e-8):\n","        super().__init__()\n","        self.alpha = alpha\n","        self.eps = eps\n","        self.cosine_similarity = nn.CosineSimilarity(dim=1, eps=eps)\n","\n","    def forward(self, y_pred, y_true):\n","        # Pearson Correlation (1 - correlation for minimization)\n","        y_pred_centered = y_pred - y_pred.mean(dim=1, keepdim=True)\n","        y_true_centered = y_true - y_true.mean(dim=1, keepdim=True)\n","        numerator = (y_pred_centered * y_true_centered).sum(dim=1)\n","        denominator = (y_pred_centered.pow(2).sum(dim=1) * y_true_centered.pow(2).sum(dim=1)).sqrt() + self.eps\n","        pearson_corr = numerator / denominator\n","        pearson_loss = 1 - pearson_corr\n","\n","        # Cosine Similarity (1 - similarity for minimization)\n","        cosine_loss = 1 - self.cosine_similarity(y_pred, y_true)\n","        loss = self.alpha * pearson_loss + (1 - self.alpha) * cosine_loss\n","        return loss.mean()\n","class HyperbolicModel(nn.Module):\n","    def __init__(self, input_dim, output_dim):\n","        super().__init__()\n","        self.fc1 = nn.Linear(input_dim, 128)\n","        self.fc2 = nn.Linear(128, 64)\n","        self.fc3 = nn.Linear(64, output_dim)\n","        self.tanh = nn.Tanh()\n","    def forward(self, x):\n","        x = self.tanh(self.fc1(x))\n","        x = self.tanh(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n","import numpy as np\n","\n","def calculate_metrics(y_true, y_pred):\n","    mse = mean_squared_error(y_true, y_pred)\n","    mae = mean_absolute_error(y_true, y_pred)\n","    r2 = r2_score(y_true, y_pred)\n","    corrs = [np.corrcoef(y_true[:,i], y_pred[:,i])[0,1] if np.std(y_true[:,i])>0 and np.std(y_pred[:,i])>0 else 0 for i in range(y_true.shape[1])]\n","    avg_pearson = np.mean(corrs)\n","    norms_true = np.linalg.norm(y_true, axis=1)\n","    norms_pred = np.linalg.norm(y_pred, axis=1)\n","    cos = np.sum(y_true * y_pred, axis=1) / (norms_true * norms_pred + 1e-8)\n","    avg_cosine = np.mean(cos)\n","    return {\n","        'MSE': mse,\n","        'MAE': mae,\n","        'R2': r2,\n","        'Avg Pearson Correlation': avg_pearson,\n","        'Avg Cosine Similarity': avg_cosine\n","    }\n","\n","# Training routine\n","# Training routine\n","def train_and_evaluate(X, y, test_ratio=0.2, epochs=100, lr=1e-3, batch_size=32, alpha=0.5):\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    # Split\n","    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_ratio, random_state=42)\n","    X_train_t = torch.FloatTensor(X_train).to(device)\n","    y_train_t = torch.FloatTensor(y_train).to(device)\n","    X_val_t = torch.FloatTensor(X_val).to(device)\n","    y_val_t = torch.FloatTensor(y_val).to(device)\n","\n","    # Model\n","    model = HyperbolicModel(X.shape[1], y.shape[1]).to(device)\n","    loss_fn = PearsonCosineLoss(alpha=alpha)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","\n","    n = X_train.shape[0]\n","    steps_per_epoch = int(np.ceil(n / batch_size))\n","\n","    # Training loop\n","    for epoch in range(epochs):\n","        model.train()\n","        idx = np.random.permutation(n)\n","        for i in range(steps_per_epoch):\n","            batch_idx = idx[i*batch_size:(i+1)*batch_size]\n","            xb = X_train_t[batch_idx]\n","            yb = y_train_t[batch_idx]\n","            optimizer.zero_grad()\n","            y_pred = model(xb)\n","            loss = loss_fn(y_pred, yb)\n","            loss.backward()\n","            optimizer.step()\n","        if (epoch+1) % 20 == 0:\n","            print(f\"Epoch {epoch+1}/{epochs}: Train Loss = {loss.item():.4f}\")\n","\n","    model.eval()\n","    with torch.no_grad():\n","        y_train_pred = model(X_train_t).cpu().numpy()\n","        y_val_pred = model(X_val_t).cpu().numpy()\n","    y_train_pred = np.clip(y_train_pred, 0.0, 5.0)  # Clip predictions to valid range\n","    y_val_pred = np.clip(y_val_pred, 0.0, 5.0)\n","\n","    # Print metrics on split data\n","    train_metrics = calculate_metrics(y_train, y_train_pred)\n","    val_metrics = calculate_metrics(y_val, y_val_pred)\n","    print(\"\\\\nTrain Metrics:\", train_metrics)\n","    print(\"Validation Metrics:\", val_metrics)\n","\n","    # Retrain on full data\n","    model = HyperbolicModel(X.shape[1], y.shape[1]).to(device)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","    X_full_t = torch.FloatTensor(X).to(device)\n","    y_full_t = torch.FloatTensor(y).to(device)\n","    for epoch in range(epochs):\n","        optimizer.zero_grad()\n","        y_pred = model(X_full_t)\n","        loss = loss_fn(y_pred, y_full_t)\n","        loss.backward()\n","        optimizer.step()\n","    model.eval()\n","    with torch.no_grad():\n","        y_full_pred = model(X_full_t).cpu().numpy()\n","    full_metrics = calculate_metrics(y, y_full_pred)\n","    print(\"\\\\nMetrics on Full Data:\", full_metrics)\n","    return model, device\n","# Run training and print metrics\n","model, device = train_and_evaluate(\n","    x_df_lasso.to_numpy(dtype=np.float32),\n","    y_df.to_numpy(dtype=np.float32),\n","    test_ratio=0.2,\n","    epochs=100,\n","    lr=1e-3,\n","    batch_size=32,\n","    alpha=0.5\n",")\n","\n","# Convert all columns to numeric (with NaNs where conversion failed)\n","X_test_numeric = X_test.apply(pd.to_numeric, errors='coerce')\n","\n","# Fill NaN values (e.g., with 0 or other imputation strategy)\n","X_test_filled = X_test_numeric.fillna(0)\n","\n","# Convert to numpy array of floats\n","X_test_arr = X_test_filled.to_numpy(dtype=np.float32)\n","with torch.no_grad():\n","    X_test_tensor = torch.FloatTensor(X_test_arr).to(device)\n","    test_preds = model(X_test_tensor).cpu().numpy()\n","test_preds = np.clip(test_preds, 0.0, 5.0)\n","# Save submission\n","test_submission = test_form[['stimulus']].copy()\n","for i, col in enumerate(target_cols):\n","    test_submission[col] = test_preds[:, i]\n","\n","test_submission.to_csv('hyperbolic_task1_embeddingpcalasso.csv', index=False)\n","print(\"Test predictions saved.\")\n","print(f\"Test submission shape: {test_submission.shape}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1754679632455,"user_tz":-330,"elapsed":1875,"user":{"displayName":"Sheerin Irfaanaa M","userId":"08172659908996108299"}},"outputId":"7f35ec0b-3564-4720-a1d1-c00ca3cf8811","id":"kIL3UfUc6RKV"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 20/100: Train Loss = 0.3015\n","Epoch 40/100: Train Loss = 0.2398\n","Epoch 60/100: Train Loss = 0.2167\n","Epoch 80/100: Train Loss = 0.1885\n","Epoch 100/100: Train Loss = 0.2150\n","\\nTrain Metrics: {'MSE': 0.0478714257478714, 'MAE': 0.11349619179964066, 'R2': 0.3727065324783325, 'Avg Pearson Correlation': np.float64(0.6634047381702715), 'Avg Cosine Similarity': np.float32(0.8559762)}\n","Validation Metrics: {'MSE': 0.07147927582263947, 'MAE': 0.15249790251255035, 'R2': 0.03350812941789627, 'Avg Pearson Correlation': np.float64(0.35279693325547656), 'Avg Cosine Similarity': np.float32(0.6869025)}\n","\\nMetrics on Full Data: {'MSE': 0.054088763892650604, 'MAE': 0.1414629966020584, 'R2': 0.2510260045528412, 'Avg Pearson Correlation': np.float64(0.5275205489660963), 'Avg Cosine Similarity': np.float32(0.7990915)}\n","Test predictions saved.\n","Test submission shape: (31, 52)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"teDpNPGH6XBD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.multioutput import MultiOutputRegressor\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n","\n","# --- Custom metrics ---\n","def pearson_correlation_score(y_true, y_pred):\n","    corrs = []\n","    for i in range(y_true.shape[1]):\n","        corr = np.corrcoef(y_true[:, i], y_pred[:, i])[0, 1]\n","        if np.isnan(corr): corr = 0\n","        corrs.append(corr)\n","    return np.mean(corrs)\n","\n","def cosine_similarity_score(y_true, y_pred):\n","    similarities = []\n","    for i in range(y_true.shape[0]):\n","        y_true_norm = y_true[i] / (np.linalg.norm(y_true[i]) + 1e-8)\n","        y_pred_norm = y_pred[i] / (np.linalg.norm(y_pred[i]) + 1e-8)\n","        similarity = np.dot(y_true_norm, y_pred_norm)\n","        similarities.append(similarity)\n","    return np.mean(similarities)\n","\n","def calculate_metrics(y_true, y_pred):\n","    return {\n","        \"MSE\": mean_squared_error(y_true, y_pred),\n","        \"MAE\": mean_absolute_error(y_true, y_pred),\n","        \"R2\": r2_score(y_true, y_pred),\n","        \"Avg Pearson Correlation\": pearson_correlation_score(y_true, y_pred),\n","        \"Avg Cosine Similarity\": cosine_similarity_score(y_true, y_pred),\n","    }\n","\n","# --------- Example workflow ---------\n","# These need to be set before running:\n","# X_df: DataFrame of features, y_df: DataFrame of targets (n_samples, n_targets)\n","# X_test_df: DataFrame of test features\n","# test_form: DataFrame with 'stimulus' column, matches rows of X_test_df\n","# target_cols: list of 51 string column names\n","\n","# 1. Convert to numpy if needed\n","X = x_df_lasso.apply(pd.to_numeric, errors='coerce').fillna(0).to_numpy(dtype=np.float32)\n","y = y_df.apply(pd.to_numeric, errors='coerce').fillna(0).to_numpy(dtype=np.float32)\n","\n","X_test = X_test.to_numpy(dtype=np.float32)\n","\n","# 2. 80/20 split\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# 3. Train MultiOutput RandomForest\n","rf = MultiOutputRegressor(RandomForestRegressor(n_estimators=100, max_depth=None, random_state=42, n_jobs=-1))\n","rf.fit(X_train, y_train)\n","\n","# 4. Print metrics on splits\n","y_train_pred = rf.predict(X_train)\n","y_val_pred = rf.predict(X_val)\n","y_train_pred = np.clip(y_train_pred, 0.0, 5.0)\n","y_val_pred = np.clip(y_val_pred, 0.0, 5.0)\n","print(\"Training metrics:\")\n","print(calculate_metrics(y_train, y_train_pred))\n","print(\"\\nValidation metrics:\")\n","print(calculate_metrics(y_val, y_val_pred))\n","\n","# 5. Retrain on entire dataset, print metrics\n","rf_full = MultiOutputRegressor(RandomForestRegressor(n_estimators=100, max_depth=None, random_state=42, n_jobs=-1))\n","rf_full.fit(X, y)\n","y_full_pred = rf_full.predict(X)\n","y_full_pred = np.clip(y_full_pred, 0.0, 5.0)\n","print(\"\\nFull dataset metrics:\")\n","print(calculate_metrics(y, y_full_pred))\n","\n","# 6. Predict on test set, save file\n","test_preds = rf_full.predict(X_test)\n","test_preds = np.clip(test_preds, 0.0, 5.0)\n","test_submission = test_form[['stimulus']].copy()\n","for i, col in enumerate(target_cols):\n","    test_submission[col] = test_preds[:, i]\n","test_submission.to_csv('rf_corr_cosine_metrics_test_task1_embeddingpcalasso.csv', index=False)\n","print(\"Test predictions saved.\")\n","print(f\"Test submission shape: {test_submission.shape}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1754679693225,"user_tz":-330,"elapsed":60739,"user":{"displayName":"Sheerin Irfaanaa M","userId":"08172659908996108299"}},"outputId":"63e4e23e-878d-481d-f865-b349fb37cd28","id":"XwmI4mWj6XxR"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training metrics:\n","{'MSE': 0.009052203125089767, 'MAE': 0.0547723981034526, 'R2': 0.8770884879439949, 'Avg Pearson Correlation': np.float64(0.9568610988065614), 'Avg Cosine Similarity': np.float64(0.9625914495615292)}\n","\n","Validation metrics:\n","{'MSE': 0.06209082806919347, 'MAE': 0.1481036948509541, 'R2': 0.06439043748311446, 'Avg Pearson Correlation': np.float64(0.43662285051861216), 'Avg Cosine Similarity': np.float64(0.725692780377936)}\n","\n","Full dataset metrics:\n","{'MSE': 0.008552602117756984, 'MAE': 0.05390139392259798, 'R2': 0.8826231514497203, 'Avg Pearson Correlation': np.float64(0.9559406065219699), 'Avg Cosine Similarity': np.float64(0.9645823953661874)}\n","Test predictions saved.\n","Test submission shape: (31, 52)\n"]}]}]}